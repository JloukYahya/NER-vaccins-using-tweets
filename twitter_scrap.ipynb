{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitter-scrap.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JloukYahya/NER-vaccins-using-tweets/blob/main/twitter_scrap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNdTFUOmBWM0"
      },
      "source": [
        "# Twitter scrapping\n",
        "# Jlouk yahya"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGc4FbSqCJDg"
      },
      "source": [
        "# imports\n",
        "import sys\n",
        "import os\n",
        "import re\n",
        "import tweepy\n",
        "from tweepy import OAuthHandler\n",
        "from textblob import TextBlob\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from IPython.display import clear_output\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "% matplotlib inline\n",
        "\n",
        "from os import path\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YaIwapFC7Yi"
      },
      "source": [
        "# install Flair\n",
        "!pip install --upgrade git+https://github.com/flairNLP/flair.git\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN7bPwceC77g"
      },
      "source": [
        "\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "tagger = SequenceTagger.load('ner')\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhUwHI1zDDs_"
      },
      "source": [
        "\n",
        "from flair.models import TextClassifier\n",
        "\n",
        "classifier = TextClassifier.load('en-sentiment')\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPfBYe-zqxme"
      },
      "source": [
        "### Twitter API\n",
        "## pour les obtenire on doit creer un compte sur twitter et passer en mode developper, apres cela il faut demander au Twitter de developer le compte pour obtenir des KPI elevated (ils reponds ont trois jours)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D82o9BhxA0tq",
        "cellView": "code"
      },
      "source": [
        "#entrer ici les kpi key et secret key\n",
        "TWITTER_KEY = '' #@param {type:\"string\"}\n",
        "TWITTER_SECRET_KEY = '' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOxCv5dKBkVz"
      },
      "source": [
        "# se connecter\n",
        "auth = tweepy.AppAuthHandler(TWITTER_KEY, TWITTER_SECRET_KEY)\n",
        "\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True,\n",
        "\t\t\t\t   wait_on_rate_limit_notify=True)\n",
        "\n",
        "if (not api):\n",
        "    print (\"Can't Authenticate\")\n",
        "    sys.exit(-1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0rweWLHXo1v"
      },
      "source": [
        "#mnt on debute notre scrapping\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "As_PRtb-Bklo"
      },
      "source": [
        "#@title chercher les APIs\n",
        "#@markdown ### le mot qu'on veut scrapper par exemple \"sinopharm\":\n",
        "searchQuery = '' #@param {type:\"string\"}\n",
        "#@markdown ### nombre de tweets qu'on veut scrapper:\n",
        "#@markdown #### la limite et de 45000 par 15minutes\n",
        "maxTweets = 90000 #@param {type:\"slider\", min:0, max:90000, step:100}\n",
        "Filter_Retweets = True #@param {type:\"boolean\"}\n",
        "\n",
        "tweetsPerQry = 100  # this is the max the API permits\n",
        "tweet_lst = []\n",
        "\n",
        "if Filter_Retweets:\n",
        "  searchQuery = searchQuery + ' -filter:retweets'  # pour ne pas avoir les retweets retweets\n",
        "\n",
        "# ici sinciId est juste pour les cas on veut scrapper plus que 45000\n",
        "#mais rester sur la meme dataset sans avoir un conflit d'id\n",
        "#donc si on voudrait scrapper plus de 45000 il faut scrapper les premier 45000 \n",
        "#attendre 15 min et specifier ici sinceId=45000, pour completer de puis l'id \n",
        "#45000\n",
        "sinceId = None\n",
        "\n",
        "max_id = -10000000000\n",
        "\n",
        "tweetCount = 0\n",
        "print(\"Downloading max {0} tweets\".format(maxTweets))\n",
        "while tweetCount < maxTweets:\n",
        "    try:\n",
        "        if (max_id <= 0):\n",
        "            if (not sinceId):\n",
        "                new_tweets = api.search(q=searchQuery, count=tweetsPerQry, lang=\"en\")\n",
        "            else:\n",
        "                new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
        "                                        lang=\"en\", since_id=sinceId)\n",
        "        else:\n",
        "            if (not sinceId):\n",
        "                new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
        "                                        lang=\"en\", max_id=str(max_id - 1))\n",
        "            else:\n",
        "                new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
        "                                        lang=\"en\", max_id=str(max_id - 1),\n",
        "                                        since_id=sinceId)\n",
        "        if not new_tweets:\n",
        "            print(\"No more tweets found\")\n",
        "            break\n",
        "        for tweet in new_tweets:\n",
        "          if hasattr(tweet, 'reply_count'):\n",
        "            reply_count = tweet.reply_count\n",
        "          else:\n",
        "            reply_count = 0\n",
        "          if hasattr(tweet, 'retweeted'):\n",
        "            retweeted = tweet.retweeted\n",
        "          else:\n",
        "            retweeted = \"NA\"\n",
        "            \n",
        "          # pour avoir le sujet on fixe le terme de recherche\n",
        "          topic = searchQuery[:searchQuery.find('-')].capitalize().strip()\n",
        "          \n",
        "          # date\n",
        "          tweetDate = tweet.created_at.date()\n",
        "          \n",
        "          tweet_lst.append([tweetDate, topic, \n",
        "                      tweet.id, tweet.user.screen_name, tweet.user.name, tweet.text, tweet.favorite_count, \n",
        "                      reply_count, tweet.retweet_count, retweeted])\n",
        "\n",
        "        tweetCount += len(new_tweets)\n",
        "        print(\"Downloaded {0} tweets\".format(tweetCount))\n",
        "        max_id = new_tweets[-1].id\n",
        "    except tweepy.TweepError as e:\n",
        "        # Just exit if any error\n",
        "        print(\"some error : \" + str(e))\n",
        "        break\n",
        "\n",
        "clear_output()\n",
        "print(\"Downloaded {0} tweets\".format(tweetCount))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVsHZlEroRQY"
      },
      "source": [
        "##Data Sciencing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CC0Lz66Jn48L"
      },
      "source": [
        "maintenant on vas stocker les tweets dans un fichier CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu7qN8q6Bkn9"
      },
      "source": [
        "pd.set_option('display.max_colwidth', -1)\n",
        "\n",
        "# utilisant pandas dataframe\n",
        "tweet_df = pd.DataFrame(tweet_lst, columns=['tweet_dt', 'topic', 'id', 'username', 'name', 'tweet', 'like_count', 'reply_count', 'retweet_count', 'retweeted'])\n",
        "tweet_df.to_csv('moderna.csv')\n",
        "tweet_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lJ8UlW3ZIsH"
      },
      "source": [
        "malheureusement twitter ne nous donne pas la possibiliter de filitrer la date\n",
        "pour cela j'ai utiliser le filitre suivant :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf_cZXTHBkqC"
      },
      "source": [
        "#@title specifier la date pour obtenire que les tweets qui date de 2021\n",
        "today = datetime.now().date()\n",
        "yesterday = today - timedelta(1)\n",
        "\n",
        "debut_dt = '2021-01-01' #@param {type:\"date\"}\n",
        "fin_dt = '2021-09-30' #@param {type:\"date\"}\n",
        "\n",
        "if start_dt == '':\n",
        "  start_dt = yesterday\n",
        "else:\n",
        "  start_dt = datetime.strptime(start_dt, '%Y-%m-%d').date()\n",
        "\n",
        "if end_dt == '':\n",
        "  end_dt = today\n",
        "else:\n",
        "  end_dt = datetime.strptime(end_dt, '%Y-%m-%d').date()\n",
        "\n",
        "\n",
        "tweet_df = tweet_df[(tweet_df['tweet_dt'] >= start_dt) \n",
        "                    & (tweet_df['tweet_dt'] <= end_dt)]\n",
        "tweet_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC-cQNXwafbt"
      },
      "source": [
        "## NER et l'analyse des sentiments\n",
        "\n",
        "pour le NER je vais utiliser Flair library: https://github.com/zalandoresearch/flair\n",
        "\n",
        "###NER\n",
        "\n",
        "precedement j'ai collocter les tweets et ajouter les tags en des lignes separer cela va nous aider par la suite. ainsi que creer un tag nommer hashtags pout que FLAIR le reconnu.\n",
        "\n",
        "### analyse des sentiments\n",
        "\n",
        "pour le KPI de polariter on utlise flair classifier et les ajouter apres a nos datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOKbfZlzBksW"
      },
      "source": [
        "# liste predection\n",
        "nerlst = []\n",
        "\n",
        "for index, row in tqdm(tweet_df.iterrows(), total=tweet_df.shape[0]):\n",
        "  cleanedTweet = row['tweet'].replace(\"#\", \"\")\n",
        "  sentence = Sentence(cleanedTweet, use_tokenizer=True)\n",
        "  \n",
        "  # predection des tags NER\n",
        "\n",
        "  tagger.predict(sentence)\n",
        "  ners = sentence.to_dict(tag_type='ner')['entities']\n",
        "  \n",
        "  # predection des sentiments\n",
        "  classifier.predict(sentence)\n",
        "  label = sentence.labels[0]\n",
        "  response = {'result': label.value, 'polarity':label.score}\n",
        "  \n",
        "  #hashtags\n",
        "  hashtags = re.findall(r'#\\w+', row['tweet'])\n",
        "  if len(hashtags) >= 1:\n",
        "    for hashtag in hashtags:\n",
        "      ners.append({ 'type': 'Hashtag', 'text': hashtag })\n",
        "  \n",
        "  for ner in ners:\n",
        "    adj_polarity = response['polarity']\n",
        "    if response['result'] == 'NEGATIVE':\n",
        "      adj_polarity = response['polarity'] * -1\n",
        "    try:\n",
        "      ner['type']\n",
        "    except:\n",
        "      ner['type'] = ''      \n",
        "    nerlst.append([ row['tweet_dt'], row['topic'], row['id'], row['username'], \n",
        "                   row['name'], row['tweet'], ner['type'], ner['text'], response['result'], \n",
        "                   response['polarity'], adj_polarity, row['like_count'], row['reply_count'], \n",
        "                  row['retweet_count'] ])\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfZVjXldBkuc"
      },
      "source": [
        "df_ner = pd.DataFrame(nerlst, columns=['tweet_dt', 'topic', 'id', 'username', 'name', 'tweet', 'tag_type', 'tag', 'sentiment', 'polarity', \n",
        "                                       'adj_polarity','like_count', 'reply_count', 'retweet_count'])\n",
        "df_ner.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETnIczIIyN_B"
      },
      "source": [
        "pour plus cerner notre dataset avant d'uiliser Openrefine ov a banner les mots qu'on veut pas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzwXUKUwBkzM",
        "cellView": "both"
      },
      "source": [
        "# filtre\n",
        "banned_words = ['anti', 'anti-vaxer', 'pharm', '#pfzer', '#johnson', 'j&j', 'vaxer', 'astral',\n",
        "                'asia', 'Asia', 'latin','latinos']\n",
        "\n",
        "df_ner = df_ner[~df_ner['tag'].isin(banned_words)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajYB9VAC4-Ca"
      },
      "source": [
        "caluclation de la frequences des tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA3E8UTwBkw6"
      },
      "source": [
        "ner_groups = df_ner.groupby(['tag', 'tag_type']).agg({'tag': \"count\", 'adj_polarity': \"mean\",\n",
        "                                                     'like_count': 'sum', 'reply_count': 'sum',\n",
        "                                                     'retweet_count': 'sum'})\n",
        "ner_groups = ner_groups.rename(columns={\n",
        "    \"tag\": \"Frequency\",\n",
        "    \"adj_polarity\": \"Avg_Polarity\",\n",
        "    \"like_count\": \"Total_Likes\",\n",
        "    \"reply_count\": \"Total_Replies\",\n",
        "    \"retweet_count\": \"Total_Retweets\"\n",
        "})\n",
        "ner_groups = ner_groups.sort_values(['Frequency'], ascending=False)\n",
        "ner_groups = ner_groups.reset_index()\n",
        "ner_groups.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inLWlkSh8IW_"
      },
      "source": [
        "creer un sentiment general soit positive ou negative\n",
        "et je mentionne la difficulter d'avoir neutre"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeBq0NeO5H3P"
      },
      "source": [
        "ner_groups['Sentiment'] = np.where(ner_groups['Avg_Polarity']>=0, 'POSITIVE', 'NEGATIVE')\n",
        "ner_groups.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLzD6bwyauz9"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbhxVDawfaEQ"
      },
      "source": [
        "on peut tracer des courbes baser sut les tags\n",
        "\n",
        "\n",
        "*   les plus populaire\n",
        "*   plus de j'aimes\n",
        "*   plus de commentaires\n",
        "*   plus de retweets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6lMQm32Bk1f",
        "cellView": "code"
      },
      "source": [
        "Filter_TAG = False \n",
        "\n",
        "#entrer le tag\n",
        "\n",
        "TAG = '' \n",
        "Top_N = 10 \n",
        "\n",
        "if TAG != 'Hashtag':\n",
        "  TAG = TAG[:3].upper()\n",
        "\n",
        "if Filter_TAG:\n",
        "  filtered_group = ner_groups[(ner_groups['tag_type'] == TAG)]\n",
        "else:\n",
        "  filtered_group = ner_groups\n",
        "\n",
        "# plot\n",
        "fig = plt.figure(figsize=(20, 16))\n",
        "fig.subplots_adjust(hspace=0.2, wspace=0.5)\n",
        "\n",
        "ax1 = fig.add_subplot(321)\n",
        "sns.barplot(x=\"Frequency\", y=\"tag\", data=filtered_group[:Top_N], hue=\"Sentiment\")\n",
        "ax2 = fig.add_subplot(322)\n",
        "filtered_group = filtered_group.sort_values(['Total_Likes'], ascending=False)\n",
        "sns.barplot(x=\"Total_Likes\", y=\"tag\", data=filtered_group[:Top_N], hue=\"Sentiment\")\n",
        "ax3 = fig.add_subplot(323)\n",
        "filtered_group = filtered_group.sort_values(['Total_Replies'], ascending=False)\n",
        "sns.barplot(x=\"Total_Replies\", y=\"tag\", data=filtered_group[:Top_N], hue=\"Sentiment\")\n",
        "ax4 = fig.add_subplot(324)\n",
        "filtered_group = filtered_group.sort_values(['Total_Retweets'], ascending=False)\n",
        "sns.barplot(x=\"Total_Retweets\", y=\"tag\", data=filtered_group[:Top_N], hue=\"Sentiment\")\n",
        "\n",
        "ax1.title.set_text('les plus populaire')\n",
        "ax2.title.set_text('les plus liker')\n",
        "ax3.title.set_text('les plis commenter')\n",
        "ax4.title.set_text('les plus retweeter')\n",
        "\n",
        "ax1.set_ylabel('')    \n",
        "ax1.set_xlabel('')\n",
        "ax2.set_ylabel('')    \n",
        "ax2.set_xlabel('')\n",
        "ax3.set_ylabel('')    \n",
        "ax3.set_xlabel('')\n",
        "ax4.set_ylabel('')    \n",
        "ax4.set_xlabel('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BybFabE9QyUv"
      },
      "source": [
        "###la polariter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRdgAEbsDLyc"
      },
      "source": [
        "fig = plt.figure(figsize=(12, 6))\n",
        "sns.distplot(filtered_group['Avg_Polarity'], hist=False, kde_kws={\"shade\": True})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipaVFUOPiJrk"
      },
      "source": [
        "\n",
        "le reste d'analyse sera avec powerBI/SPSS"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merci.\n"
      ],
      "metadata": {
        "id": "UMt5t5xB6Rr6"
      }
    }
  ]
}